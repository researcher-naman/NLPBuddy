{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8235eb8-8887-4136-a78b-7a0cd52d8884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Attention, Bidirectional, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a776755b-ae0f-4627-b21b-b778c9297dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"semeval2014.csv\")\n",
    "\n",
    "# Encode polarity labels (positive=2, neutral=1, negative=0)\n",
    "label_encoder = LabelEncoder()\n",
    "data['polarity'] = label_encoder.fit_transform(data['polarity'])\n",
    "\n",
    "# Tokenize text and aspects\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(data['Sentence'])\n",
    "\n",
    "# Convert reviews and aspects to sequences\n",
    "review_sequences = tokenizer.texts_to_sequences(data['Sentence'])\n",
    "aspect_sequences = tokenizer.texts_to_sequences(data['Aspect Term'])\n",
    "\n",
    "# Pad sequences\n",
    "max_seq_len = 100  # Maximum length for reviews\n",
    "review_padded = pad_sequences(review_sequences, maxlen=max_seq_len, padding='post', truncating='post')\n",
    "aspect_padded = pad_sequences(aspect_sequences, maxlen=max_seq_len, padding='post', truncating='post')\n",
    "\n",
    "# Extract labels\n",
    "labels = tf.keras.utils.to_categorical(data['polarity'], num_classes=4)\n",
    "\n",
    "# Split into training and testing datasets\n",
    "train_reviews, test_reviews, train_aspects, test_aspects, train_labels, test_labels = train_test_split(\n",
    "    review_padded, aspect_padded, labels, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f8d229b-f058-454e-ac31-1ead16ef570a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naman\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naman\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_45', 'keras_tensor_46']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 74ms/step - accuracy: 0.4227 - loss: 1.1852 - val_accuracy: 0.4025 - val_loss: 1.1169\n",
      "Epoch 2/5\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 68ms/step - accuracy: 0.5563 - loss: 1.0343 - val_accuracy: 0.6208 - val_loss: 0.9015\n",
      "Epoch 3/5\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 69ms/step - accuracy: 0.7361 - loss: 0.7042 - val_accuracy: 0.7055 - val_loss: 0.8174\n",
      "Epoch 4/5\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 68ms/step - accuracy: 0.8343 - loss: 0.4779 - val_accuracy: 0.7034 - val_loss: 0.8445\n",
      "Epoch 5/5\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 66ms/step - accuracy: 0.8678 - loss: 0.3783 - val_accuracy: 0.7013 - val_loss: 0.9107\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7391 - loss: 0.7982\n",
      "Test Loss: 0.9106934070587158\n",
      "Test Accuracy: 0.7012711763381958\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Layer, Input, Embedding, Bidirectional, LSTM, Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the AttentionLayer\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, lstm_units):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.lstm_units = lstm_units\n",
    "        self.lstm = Bidirectional(LSTM(self.lstm_units, return_sequences=True))  # Create LSTM layer here\n",
    "\n",
    "    def call(self, inputs):\n",
    "        text_embedded, aspect_embedded = inputs\n",
    "        \n",
    "        # Process the text using a Bi-directional LSTM\n",
    "        lstm_out = self.lstm(text_embedded)  # Apply LSTM to text embeddings\n",
    "        \n",
    "        # Aspect representation as query\n",
    "        query = tf.reduce_mean(aspect_embedded, axis=1)  # shape: (batch_size, 128)\n",
    "        \n",
    "        # Ensure the query shape is compatible for matmul\n",
    "        query = tf.expand_dims(query, axis=1)  # shape: (batch_size, 1, 128)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_scores = tf.matmul(lstm_out, query, transpose_b=True)  # shape: (batch_size, seq_len, 1)\n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=1)\n",
    "        \n",
    "        # Weighted sum of LSTM outputs based on attention weights\n",
    "        context_vector = tf.reduce_sum(attention_weights * lstm_out, axis=1)  # shape: (batch_size, lstm_units * 2)\n",
    "        \n",
    "        return context_vector\n",
    "\n",
    "# Model Architecture\n",
    "def build_model(vocab_size, embedding_dim, max_seq_len, lstm_units):\n",
    "    # Input layers\n",
    "    review_input = Input(shape=(max_seq_len,))\n",
    "    aspect_input = Input(shape=(max_seq_len,))\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_len)\n",
    "    review_embedded = embedding_layer(review_input)\n",
    "    aspect_embedded = embedding_layer(aspect_input)\n",
    "    \n",
    "    # Attention layer\n",
    "    attention_layer = AttentionLayer(lstm_units)\n",
    "    context_vector = attention_layer([review_embedded, aspect_embedded])\n",
    "    \n",
    "    # Fully connected layer\n",
    "    dense_layer = Dense(64, activation='relu')(context_vector)\n",
    "    output_layer = Dense(4, activation='softmax')(dense_layer)  # 4 classes for polarity\n",
    "\n",
    "    # Model\n",
    "    model = tf.keras.models.Model(inputs=[review_input, aspect_input], outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = 10000  # You can use the tokenizer's word index size\n",
    "embedding_dim = 128\n",
    "max_seq_len = 100\n",
    "lstm_units = 64\n",
    "\n",
    "# Build the model\n",
    "model = build_model(vocab_size, embedding_dim, max_seq_len, lstm_units)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [train_reviews, train_aspects], \n",
    "    train_labels, \n",
    "    epochs=5, \n",
    "    batch_size=32, \n",
    "    validation_data=([test_reviews, test_aspects], test_labels)\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate([test_reviews, test_aspects], test_labels)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c8a68-9311-4062-a355-ee2297868183",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67f17499-f865-487d-bef3-ee84315f1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample test data (pre-tokenized and padded)\n",
    "sample_reviews = [\n",
    "    \"The camera quality is amazing and the battery life is great.\",  # Review 1\n",
    "    \"The screen resolution is poor, but the performance is decent.\",  # Review 2\n",
    "]\n",
    "\n",
    "sample_aspects = [\n",
    "    \"camera\",  # Aspect for Review 1\n",
    "    \"performance\",  # Aspect for Review 2\n",
    "]\n",
    "\n",
    "# Convert these to padded sequences (you need to use the same tokenizer and padding as during training)\n",
    "# Assuming `tokenizer` is already defined during your training process\n",
    "max_seq_len = 100  # Use the same max length as used during training\n",
    "\n",
    "sample_reviews_seq = tokenizer.texts_to_sequences(sample_reviews)\n",
    "sample_reviews_padded = pad_sequences(sample_reviews_seq, maxlen=max_seq_len, padding='post', truncating='post')\n",
    "\n",
    "sample_aspects_seq = tokenizer.texts_to_sequences(sample_aspects)\n",
    "sample_aspects_padded = pad_sequences(sample_aspects_seq, maxlen=max_seq_len, padding='post', truncating='post')\n",
    "\n",
    "# Convert to numpy arrays for model input\n",
    "import numpy as np\n",
    "sample_reviews_padded = np.array(sample_reviews_padded)\n",
    "sample_aspects_padded = np.array(sample_aspects_padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "206d1ce7-fda2-48e3-9708-b81afee5a73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step\n",
      "Review: The camera quality is amazing and the battery life is great.\n",
      "Aspect: camera\n",
      "Predicted Sentiment: Mixed\n",
      "--------------------------------------------------\n",
      "Review: The screen resolution is poor, but the performance is decent.\n",
      "Aspect: performance\n",
      "Predicted Sentiment: Negative\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the sample data\n",
    "predictions = model.predict([sample_reviews_padded, sample_aspects_padded])\n",
    "\n",
    "# Convert predictions to readable class labels (assuming one-hot encoding with 4 classes)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Map the predicted classes back to the corresponding sentiment labels (e.g., 0: Positive, 1: Negative, 2: Neutral, 3: Mixed)\n",
    "sentiment_labels = ['Positive', 'Negative', 'Neutral', 'Mixed']  # Adjust as needed based on your model's output\n",
    "predicted_sentiments = [sentiment_labels[class_idx] for class_idx in predicted_classes]\n",
    "\n",
    "# Display the results\n",
    "for review, aspect, sentiment in zip(sample_reviews, sample_aspects, predicted_sentiments):\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Aspect: {aspect}\")\n",
    "    print(f\"Predicted Sentiment: {sentiment}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb34ca-5040-4db5-9fe2-606d10eb0b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
